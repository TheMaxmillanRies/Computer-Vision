{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd62d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "#from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from glob import glob\n",
    "from os import listdir\n",
    "from os.path import *\n",
    "from PIL import Image, ImageOps, ImageFile\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed2eba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "#from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from glob import glob\n",
    "from os import listdir\n",
    "from os.path import *\n",
    "from PIL import Image, ImageOps, ImageFile\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef17b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### RAE Encoder Definitions #######################################################\n",
    "\n",
    "# reference to code: https://towardsdev.com/implement-resnet-with-pytorch-a9fb40a77448\n",
    "# https://github.com/Alvinhech/resnet-autoencoder/blob/cdcaab6c6c9792f76f46190c2b6407a28702f7af/autoencoder1.py#L142\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        output.append(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        output.append(out)  \n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "      \n",
    "        output.append(out)  \n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "        output.append(out)  \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "172171d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### RAE Decoder Definitions #######################################################\n",
    "\n",
    "class DeconvBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=2, stride=1, upsample=None):\n",
    "        super(DeconvBottleneck, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        if stride == 1:\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                                   stride=stride, bias=False, padding=1)\n",
    "        else:\n",
    "            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=stride, bias=False,\n",
    "                                            padding=1,\n",
    "                                            output_padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            shortcut = self.upsample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "866308ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### ResNet Encoder Definition #######################################################\n",
    "\n",
    "class ResNet_encoder(nn.Module):\n",
    "    def __init__(self, downblock, num_layers):\n",
    "        super(ResNet_encoder, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n",
    "        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1],\n",
    "                                           stride=2)\n",
    "        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2],\n",
    "                                           stride=2)\n",
    "        self.layer4 = self._make_downlayer(downblock, 512, num_layers[3],\n",
    "                                           stride=2)\n",
    "\n",
    "\n",
    "    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != init_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, init_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(init_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, init_channels, stride, downsample))\n",
    "        self.in_channels = init_channels * block.expansion\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        output['re-l0-1'] = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        output_layer1 = self.layer1(x)\n",
    "        x = output_layer1[3]\n",
    "        output['re-l1-1'] = output_layer1[0]\n",
    "        output['re-l1-2'] = output_layer1[1]\n",
    "        output['re-l1-2'] = output_layer1[2]\n",
    "        output['re-l1-3'] = output_layer1[3]\n",
    "\n",
    "        output_layer2 = self.layer2(x)\n",
    "        x = output_layer2[3]\n",
    "        output['re-l2-1'] = output_layer2[0]\n",
    "        output['re-l2-2'] = output_layer2[1]\n",
    "        output['re-l2-2'] = output_layer2[2]\n",
    "        output['re-l2-3'] = output_layer2[3]\n",
    "\n",
    "        output_layer3 = self.layer3(x)\n",
    "        x = output_layer3[3]\n",
    "        output['re-l3-1'] = output_layer3[0]\n",
    "        output['re-l3-2'] = output_layer3[1]\n",
    "        output['re-l3-2'] = output_layer3[2]\n",
    "        output['re-l3-3'] = output_layer3[3]\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        output_layer4 = self.layer4(x)\n",
    "        x = output_layer4[3]\n",
    "        output['re-l4-1'] = output_layer4[0]\n",
    "        output['re-l4-2'] = output_layer4[1]\n",
    "        output['re-l4-2'] = output_layer4[2]\n",
    "        output['re-l4-3'] = output_layer4[3]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f92105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### ResNet Decoder Definition #######################################################\n",
    "\n",
    "class ResNet_decoder(nn.Module):\n",
    "    def __init__(self, upblock, num_layers, n_classes):\n",
    "        super(ResNet_decoder, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.uplayer1 = self._make_up_block(\n",
    "            upblock, 512,  num_layers[3], stride=2)\n",
    "        self.uplayer2 = self._make_up_block(\n",
    "            upblock, 256, num_layers[2], stride=2)\n",
    "        self.uplayer3 = self._make_up_block(\n",
    "            upblock, 128, num_layers[1], stride=2)\n",
    "        self.uplayer4 = self._make_up_block(\n",
    "            upblock, 64,  num_layers[0], stride=2)\n",
    "\n",
    "        upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.in_channels,  # 256\n",
    "                               64,\n",
    "                               kernel_size=1, stride=2,\n",
    "                               bias=False, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.uplayer_top = DeconvBottleneck(\n",
    "            self.in_channels, 64, 1, 2, upsample)\n",
    "\n",
    "        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1,\n",
    "                                          bias=False)\n",
    "\n",
    "    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n",
    "        upsample = None\n",
    "        # expansion = block.expansion\n",
    "        if stride != 1 or self.in_channels != init_channels * 2:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.in_channels, init_channels * 2,\n",
    "                                   kernel_size=1, stride=stride,\n",
    "                                   bias=False, output_padding=1),\n",
    "                nn.BatchNorm2d(init_channels * 2),\n",
    "            )\n",
    "        layers = []\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels, 4))\n",
    "        layers.append(\n",
    "            block(self.in_channels, init_channels, 2, stride, upsample))\n",
    "        self.in_channels = init_channels * 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, image_size):\n",
    "        x = self.uplayer1(x)\n",
    "        x = self.uplayer2(x)\n",
    "        x = self.uplayer3(x)\n",
    "        x = self.uplayer4(x)\n",
    "\n",
    "        x = self.conv1_1(x, output_size=image_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a745218",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We do not need the forward method  \n",
    "\n",
    "class ResNet_autoencoder(nn.Module):\n",
    "    def __init__(self, downblock, upblock, num_layers, n_classes):\n",
    "        super(ResNet_autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = ResNet_encoder(downblock, num_layers)\n",
    "        self.decoder = ResNet_decoder(upblock, num_layers, n_classes)\n",
    "\n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        return self.encoder.forward(x)\n",
    "\n",
    "    def decoder(self, x, image_size):\n",
    "        return self.decoder.forward(x, image_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = x\n",
    "        dict_encoder = self.encoder(x)\n",
    "        # final value of the encoder is stored in entry 're-l4-3'\n",
    "        x = self.decoder(dict_encoder['re-l4-3'], img.size())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ae24c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, matrixSize=32):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(nn.Conv2d(512,256,3,1,1),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(256,128,3,1,1),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(128,matrixSize,3,1,1))\n",
    "\n",
    "        # 32x8x8\n",
    "        self.fc = nn.Linear(matrixSize*matrixSize,matrixSize*matrixSize)\n",
    "        #self.fc = nn.Linear(32*64,256*256)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.convs(x)\n",
    "        # 32x8x8\n",
    "        b,c,h,w = out.size()\n",
    "        out = out.view(b,c,-1)\n",
    "        # 32x64\n",
    "        out = torch.bmm(out,out.transpose(1,2)).div(h*w)\n",
    "        # 32x32\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return self.fc(out)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        # 32x8x8\n",
    "        self.encode = nn.Sequential(nn.Linear(512, 2*z_dim),\n",
    "                                    )\n",
    "        self.bn = nn.BatchNorm1d(z_dim)\n",
    "        self.decode = nn.Sequential(nn.Linear(z_dim, 512),\n",
    "                                    nn.BatchNorm1d(512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(512, 512),\n",
    "                                    )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        mu = self.bn(mu)\n",
    "        std = torch.exp(logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + std\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 32x8x8\n",
    "        b,c,h = x.size()\n",
    "        x = x.view(b,-1)\n",
    "\n",
    "        z_q_mu, z_q_logvar = self.encode(x).chunk(2, dim=1)\n",
    "        # reparameterize\n",
    "        z_q = self.reparameterize(z_q_mu, z_q_logvar)\n",
    "        out = self.decode(z_q)\n",
    "        out = out.view(b,c,h)\n",
    "\n",
    "        KL = torch.sum(0.5 * (z_q_mu.pow(2) + z_q_logvar.exp().pow(2) - 1) - z_q_logvar)\n",
    "\n",
    "        return out, KL\n",
    "\n",
    "class MulLayer(nn.Module):\n",
    "    def __init__(self, z_dim, matrixSize=32):\n",
    "        super(MulLayer,self).__init__()\n",
    "        # self.snet = CNN_VAE(layer, z_dim, matrixSize)\n",
    "        self.snet = CNN(matrixSize)\n",
    "        self.cnet = CNN(matrixSize)\n",
    "        self.VAE = VAE(z_dim=z_dim)\n",
    "        self.matrixSize = matrixSize\n",
    "\n",
    "        self.compress = nn.Conv2d(512,matrixSize,1,1,0)\n",
    "        self.unzip = nn.Conv2d(matrixSize,512,1,1,0)\n",
    "\n",
    "        self.transmatrix = None\n",
    "\n",
    "    def forward(self,cF,sF,trans=True):\n",
    "        cb,cc,ch,cw = cF.size()\n",
    "        cFF = cF.view(cb,cc,-1)\n",
    "        cMean = torch.mean(cFF,dim=2,keepdim=True)\n",
    "        cMean = cMean.unsqueeze(3)\n",
    "        cMean = cMean.expand_as(cF)\n",
    "        cF = cF - cMean\n",
    "\n",
    "        sb,sc,sh,sw = sF.size()\n",
    "        sFF = sF.view(sb,sc,-1)\n",
    "        sMean = torch.mean(sFF,dim=2,keepdim=True)\n",
    "        sMean, KL = self.VAE(sMean)\n",
    "        sMean = sMean.unsqueeze(3)\n",
    "        sMeanC = sMean.expand_as(cF)\n",
    "        sMeanS = sMean.expand_as(sF)\n",
    "        sF = sF - sMeanS\n",
    "\n",
    "\n",
    "        compress_content = self.compress(cF)\n",
    "        b,c,h,w = compress_content.size()\n",
    "        compress_content = compress_content.view(b,c,-1)\n",
    "\n",
    "        if(trans):\n",
    "            cMatrix = self.cnet(cF)\n",
    "            sMatrix = self.snet(sF)\n",
    "\n",
    "            sMatrix = sMatrix.view(sMatrix.size(0),self.matrixSize,self.matrixSize)\n",
    "            cMatrix = cMatrix.view(cMatrix.size(0),self.matrixSize,self.matrixSize)\n",
    "            transmatrix = torch.bmm(sMatrix,cMatrix)\n",
    "            transfeature = torch.bmm(transmatrix,compress_content).view(b,c,h,w)\n",
    "            out = self.unzip(transfeature.view(b,c,h,w))\n",
    "            out = out + sMeanC\n",
    "            return out, transmatrix, KL\n",
    "        else:\n",
    "            out = self.unzip(compress_content.view(b,c,h,w))\n",
    "            out = out + cMean\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6f44afcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class styleLoss(nn.Module):\n",
    "    def forward(self,input,target):\n",
    "        ib,ic,ih,iw = input.size()\n",
    "        iF = input.view(ib,ic,-1)\n",
    "        iMean = torch.mean(iF,dim=2)\n",
    "        iCov = GramMatrix()(input)\n",
    "\n",
    "        tb,tc,th,tw = target.size()\n",
    "        tF = target.view(tb,tc,-1)\n",
    "        tMean = torch.mean(tF,dim=2)\n",
    "        tCov = GramMatrix()(target)\n",
    "\n",
    "        loss = nn.MSELoss(size_average=False)(iMean,tMean) + nn.MSELoss(size_average=False)(iCov,tCov)\n",
    "        return loss/tb\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self,input):\n",
    "        b, c, h, w = input.size()\n",
    "        f = input.view(b,c,h*w) # bxcx(hxw)\n",
    "        # torch.bmm(batch1, batch2, out=None)   #\n",
    "        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n",
    "        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n",
    "        return G.div_(c*h*w)\n",
    "\n",
    "class LossCriterion(nn.Module):\n",
    "    def __init__(self,style_layers,content_layers,style_weight,content_weight):\n",
    "        super(LossCriterion,self).__init__()\n",
    "\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.style_weight = style_weight\n",
    "        self.content_weight = content_weight\n",
    "\n",
    "        self.styleLosses = [styleLoss()] * len(style_layers)\n",
    "        self.contentLosses = [nn.MSELoss()] * len(content_layers)\n",
    "\n",
    "    def forward(self, tF, sF, cF, KL):\n",
    "        # content loss\n",
    "        totalContentLoss = 0\n",
    "        for i,layer in enumerate(self.content_layers):\n",
    "            cf_i = cF[layer]\n",
    "            cf_i = cf_i.detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.contentLosses[i]\n",
    "            totalContentLoss += loss_i(tf_i,cf_i)\n",
    "        totalContentLoss = totalContentLoss * self.content_weight\n",
    "\n",
    "        # style loss\n",
    "        totalStyleLoss = 0\n",
    "        for i,layer in enumerate(self.style_layers):\n",
    "            sf_i = sF[layer]\n",
    "            sf_i = sf_i.detach()\n",
    "            tf_i = tF[layer]\n",
    "            loss_i = self.styleLosses[i]\n",
    "            totalStyleLoss += loss_i(tf_i,sf_i)\n",
    "        totalStyleLoss = totalStyleLoss * self.style_weight\n",
    "\n",
    "        # KL loss\n",
    "        KL = torch.sum(KL)\n",
    "\n",
    "        # laplacian loss\n",
    "        # Laploss = Lap_criterion(2*ori_content-1, 2*ori_style-1)\n",
    "\n",
    "        # total loss\n",
    "        loss = totalStyleLoss + totalContentLoss + KL\n",
    "\n",
    "        return loss, totalStyleLoss, totalContentLoss, KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ee2c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder5,self).__init__()\n",
    "        # vgg\n",
    "        # 224 x 224\n",
    "        self.conv1 = nn.Conv2d(3,3,1,1,0)\n",
    "        self.reflecPad1 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        # 226 x 226\n",
    "\n",
    "        self.conv2 = nn.Conv2d(3,64,3,1,0)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        # 224 x 224\n",
    "\n",
    "        self.reflecPad3 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv3 = nn.Conv2d(64,64,3,1,0)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        # 224 x 224\n",
    "\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # 112 x 112\n",
    "\n",
    "        self.reflecPad4 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3,1,0)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        # 112 x 112\n",
    "\n",
    "        self.reflecPad5 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv5 = nn.Conv2d(128,128,3,1,0)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        # 112 x 112\n",
    "\n",
    "        self.maxPool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # 56 x 56\n",
    "\n",
    "        self.reflecPad6 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv6 = nn.Conv2d(128,256,3,1,0)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        # 56 x 56\n",
    "\n",
    "        self.reflecPad7 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv7 = nn.Conv2d(256,256,3,1,0)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        # 56 x 56\n",
    "\n",
    "        self.reflecPad8 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv8 = nn.Conv2d(256,256,3,1,0)\n",
    "        self.relu8 = nn.ReLU(inplace=True)\n",
    "        # 56 x 56\n",
    "\n",
    "        self.reflecPad9 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv9 = nn.Conv2d(256,256,3,1,0)\n",
    "        self.relu9 = nn.ReLU(inplace=True)\n",
    "        # 56 x 56\n",
    "\n",
    "        self.maxPool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        # 28 x 28\n",
    "\n",
    "        self.reflecPad10 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv10 = nn.Conv2d(256,512,3,1,0)\n",
    "        self.relu10 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.reflecPad11 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv11 = nn.Conv2d(512,512,3,1,0)\n",
    "        self.relu11 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.reflecPad12 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv12 = nn.Conv2d(512,512,3,1,0)\n",
    "        self.relu12 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.reflecPad13 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv13 = nn.Conv2d(512,512,3,1,0)\n",
    "        self.relu13 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxPool4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.reflecPad14 = nn.ReflectionPad2d((1,1,1,1))\n",
    "        self.conv14 = nn.Conv2d(512,512,3,1,0)\n",
    "        self.relu14 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self,x,sF=None,contentV256=None,styleV256=None,matrix11=None,matrix21=None,matrix31=None):\n",
    "        output = {}\n",
    "        out = self.conv1(x)\n",
    "        out = self.reflecPad1(out)\n",
    "        out = self.conv2(out)\n",
    "        output['r11'] = self.relu2(out)\n",
    "        out = self.reflecPad7(output['r11'])\n",
    "\n",
    "        #out = self.reflecPad3(output['r11'])\n",
    "        out = self.conv3(out)\n",
    "        output['r12'] = self.relu3(out)\n",
    "\n",
    "        output['p1'] = self.maxPool(output['r12'])\n",
    "        out = self.reflecPad4(output['p1'])\n",
    "        out = self.conv4(out)\n",
    "        output['r21'] = self.relu4(out)\n",
    "        out = self.reflecPad7(output['r21'])\n",
    "\n",
    "        #out = self.reflecPad5(output['r21'])\n",
    "        out = self.conv5(out)\n",
    "        output['r22'] = self.relu5(out)\n",
    "\n",
    "        output['p2'] = self.maxPool2(output['r22'])\n",
    "        out = self.reflecPad6(output['p2'])\n",
    "        out = self.conv6(out)\n",
    "        output['r31'] = self.relu6(out)\n",
    "        if(styleV256 is not None):\n",
    "            feature = matrix31(output['r31'],sF['r31'],contentV256,styleV256)\n",
    "            out = self.reflecPad7(feature)\n",
    "        else:\n",
    "            out = self.reflecPad7(output['r31'])\n",
    "        out = self.conv7(out)\n",
    "        output['r32'] = self.relu7(out)\n",
    "\n",
    "        out = self.reflecPad8(output['r32'])\n",
    "        out = self.conv8(out)\n",
    "        output['r33'] = self.relu8(out)\n",
    "\n",
    "        out = self.reflecPad9(output['r33'])\n",
    "        out = self.conv9(out)\n",
    "        output['r34'] = self.relu9(out)\n",
    "\n",
    "        output['p3'] = self.maxPool3(output['r34'])\n",
    "        out = self.reflecPad10(output['p3'])\n",
    "        out = self.conv10(out)\n",
    "        output['r41'] = self.relu10(out)\n",
    "\n",
    "        out = self.reflecPad11(output['r41'])\n",
    "        out = self.conv11(out)\n",
    "        output['r42'] = self.relu11(out)\n",
    "\n",
    "        out = self.reflecPad12(output['r42'])\n",
    "        out = self.conv12(out)\n",
    "        output['r43'] = self.relu12(out)\n",
    "\n",
    "        out = self.reflecPad13(output['r43'])\n",
    "        out = self.conv13(out)\n",
    "        output['r44'] = self.relu13(out)\n",
    "\n",
    "        output['p4'] = self.maxPool4(output['r44'])\n",
    "\n",
    "        out = self.reflecPad14(output['p4'])\n",
    "        out = self.conv14(out)\n",
    "        output['r51'] = self.relu14(out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54df7f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################### Training Loop #######################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44bde982",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = Image.open(filepath).convert('RGB')\n",
    "    # y, _, _ = img.split()\n",
    "    return img\n",
    "\n",
    "\n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, data_dir, ref_dir, fineSize=256):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.ref_dir = ref_dir\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((288, 288)),\n",
    "            transforms.RandomCrop(fineSize),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "        self.style_transform = transforms.Compose([\n",
    "            transforms.Resize((fineSize, fineSize)),\n",
    "            # transforms.RandomCrop(fineSize),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor()])\n",
    "\n",
    "\n",
    "        self.input_filenames = sorted(glob(join(data_dir, '*.jpg')))\n",
    "        self.ref_filenames = sorted(glob(join(ref_dir, '*/*.jpg')))\n",
    "        self.ref_len = len(self.ref_filenames)\n",
    "        self.input_len = len(self.input_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = load_img(self.input_filenames[index])\n",
    "        rand_no = torch.randint(0, self.ref_len, (1,)).item()\n",
    "        ref = load_img(self.ref_filenames[rand_no])\n",
    "\n",
    "\n",
    "        input = self.transform(input)\n",
    "        ref = self.style_transform(ref)\n",
    "\n",
    "        return input, ref\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.input_len\n",
    "\n",
    "\n",
    "def transform():\n",
    "    return transforms.Compose([\n",
    "        # ColorJitter(hue=0.3, brightness=0.3, saturation=0.3),\n",
    "        # RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_training_set(data_dir):\n",
    "    content_dir = data_dir + '/content'\n",
    "    ref_dir = data_dir + '/style'\n",
    "    train_set = DatasetFromFolder(data_dir, ref_dir)\n",
    "\n",
    "    # Pytorch train and test sets\n",
    "    # tensor_dataset = torch.utils.data.TensorDataset(train_set)\n",
    "\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def get_testing_set(test_dir, data_augmentation):\n",
    "\n",
    "    test_set = DatasetFromFolder(test_dir, data_augmentation, transform=transform())\n",
    "\n",
    "    # Pytorch train and test sets\n",
    "    # tensor_dataset = torch.utils.data.TensorDataset(train_set)\n",
    "\n",
    "    return test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c16f2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING PARAMETERS\n",
    "\n",
    "style_layers = ['re-l1-1', 're-l1-2']\n",
    "content_layers = ['re-l1-1']\n",
    "style_weight = 0.2\n",
    "content_weight = 0.2\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 1\n",
    "threads = 1\n",
    "\n",
    "latent_dim = 33 # dummy value\n",
    "\n",
    "data_dir = './train_data' # download from https://cocodataset.org/#home \n",
    "\n",
    "\n",
    "resnet_dir = './resnet50-19c8e357.pth'\n",
    "loss_network_dir = './vgg_r51.pth'\n",
    "\n",
    "start_iter = 0\n",
    "num_epochs = 1\n",
    "snapshots = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "798a3327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading datasets\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 92>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===> Epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Complete: Avg. Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(training_data_loader)))\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content, style, transfer\n\u001b[0;32m---> 92\u001b[0m content, style, transfer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m'''for epoch in range(start_iter, _epochs + 1):\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    content, style, transfer = train(epoch)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        torch.save(matrix.state_dict(), '%s/%s_epoch_%d.pth' % (opt.outf, opt.layer, epoch))'''\u001b[39;00m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(iteration)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m#print(\"===> Epoch[{}]({}/{}): loss: {:.4f} || content: {:.4f} || style: {:.4f} KL: {:.4f}.\".format(epoch, iteration, len(training_data_loader), loss, contentLoss, styleLoss, KL_loss,))\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===> Epoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Complete: Avg. Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, \u001b[43mepoch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, style, transfer\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Create the model - The array is the number of blocks -  and the 3 is (we think) RGB \n",
    "model = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).cuda()\n",
    "\n",
    "################# DATA LOADER #################\n",
    "print('===> Loading datasets')\n",
    "train_set = get_training_set(data_dir)\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=threads, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# chose the latent space\n",
    "matrix = MulLayer(z_dim=latent_dim)\n",
    "\n",
    "# Load a pretrained ResNet 50 \n",
    "pretrained_dict = torch.load(resnet_dir)\n",
    "print(\"load pretrained model success\")\n",
    "\n",
    "\n",
    "# Create model dictionary\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# Set all parameters to untrainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "################# LOSS & OPTIMIZER #################\n",
    "# Possible entries \n",
    "criterion = LossCriterion(style_layers, content_layers, style_weight, content_weight)\n",
    "optimizer = optim.Adam(matrix.parameters(), learning_rate)\n",
    "\n",
    "vgg5 = encoder5() # Loss network\n",
    "\n",
    "vgg5.load_state_dict(torch.load(loss_network_dir))\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        content, target, style = Variable(batch[0]), Variable(batch[1]), Variable(batch[2])\n",
    "        content = content.cuda()\n",
    "        target = target.cuda()\n",
    "        style = style.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        ## x is the input \n",
    "        sF = model.encoder(style)\n",
    "        cF = model.encoder(style)\n",
    "\n",
    "        # possible values in the dictionary of the encoder per layer: \n",
    "        # first relu after the first conolution: 're-l0-1'\n",
    "        # layer 1: 're-l1-1' -> 're-l1-2' -> 're-l1-3' -> 're-l1-4'\n",
    "        # layer 2: 're-l2-1' -> 're-l2-2' -> 're-l2-3' -> 're-l2-4'\n",
    "        # layer 3: 're-l3-1' -> 're-l3-2' -> 're-l3-3' -> 're-l3-4'\n",
    "        # layer 4: 're-l4-1' -> 're-l4-2' -> 're-l4-3' -> 're-l4-4'\n",
    "        # this needs to be carefully chosen \n",
    "        cF_intermediate = cF['re-l1-1']\n",
    "        sF_intermediate = sF['re-l1-1', 're-l1-2']\n",
    "        feature, transmatrix, KL = matrix(cF_intermediate, sF_intermediate)\n",
    "\n",
    "\n",
    "        transfer = model.decoder(feature)\n",
    "\n",
    "\n",
    "        ## Need to find a suitable loss network \n",
    "        sF_loss = vgg5(style)\n",
    "        cF_loss = vgg5(content)\n",
    "        tF = vgg5(transfer)\n",
    "        loss, styleLoss, contentLoss, KL_loss = criterion(tF, sF_loss, cF_loss, KL)\n",
    "\n",
    "        # backward & optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        print(\"===> Epoch[{}]({}/{}): loss: {:.4f} || content: {:.4f} || style: {:.4f} KL: {:.4f}.\".format(epoch, iteration, len(training_data_loader), loss, contentLoss, styleLoss, KL_loss,))\n",
    "\n",
    "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n",
    "\n",
    "    return content, style, transfer\n",
    "\n",
    "\n",
    "for epoch in range(start_iter, _epochs + 1):\n",
    "    content, style, transfer = train(epoch)\n",
    "\n",
    "    # learning rate is decayed by a factor of 10 every half of total epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] /= 10.0\n",
    "        print('Learning rate decay: lr={}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    if epoch % (snapshots) == 0:\n",
    "        content = content.clamp(0, 1).cpu().data\n",
    "        style = style.clamp(0, 1).cpu().data\n",
    "        transfer = transfer.clamp(0, 1).cpu().data\n",
    "        concat = torch.cat((content, style, transfer), dim=0)\n",
    "        vutils.save_image(concat, '%s/%d.png' % (opt.outf, epoch), normalize=True, scale_each=True, nrow=batch_size)\n",
    "\n",
    "        torch.save(matrix.state_dict(), '%s/%s_epoch_%d.pth' % (opt.outf, opt.layer, epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1cc3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297ba74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c286eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
