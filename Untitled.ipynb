{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "#from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### RAE Encoder Definitions #######################################################\n",
    "\n",
    "# reference to code: https://towardsdev.com/implement-resnet-with-pytorch-a9fb40a77448\n",
    "# https://github.com/Alvinhech/resnet-autoencoder/blob/cdcaab6c6c9792f76f46190c2b6407a28702f7af/autoencoder1.py#L142\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = []shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "                output.append(out)\n",
    "out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "      \n",
    "        output.append(out)  out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "      \n",
    "        output.append(out)  if self.downsample is not None:\n",
    "            shortcut = self.downsample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "      \n",
    "        output.append(out)  return out\n",
    "     output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172171d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### RAE Decoder Definitions #######################################################\n",
    "\n",
    "class DeconvBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion=2, stride=1, upsample=None):\n",
    "        super(DeconvBottleneck, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        if stride == 1:\n",
    "            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                                   stride=stride, bias=False, padding=1)\n",
    "        else:\n",
    "            self.conv2 = nn.ConvTranspose2d(out_channels, out_channels,\n",
    "                                            kernel_size=3,\n",
    "                                            stride=stride, bias=False,\n",
    "                                            padding=1,\n",
    "                                            output_padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, x):shortcut = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "            shortcut = self.upsample(x)\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "     out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866308ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### ResNet Encoder Definition #######################################################\n",
    "\n",
    "class ResNet_encoder(nn.Module):\n",
    "    def __init__(self, downblock, num_layers):\n",
    "        super(ResNet_encoder, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_downlayer(downblock, 64, num_layers[0])\n",
    "        self.layer2 = self._make_downlayer(downblock, 128, num_layers[1],\n",
    "                                           stride=2)\n",
    "        self.layer3 = self._make_downlayer(downblock, 256, num_layers[2],\n",
    "                                           stride=2)\n",
    "        self.layer4 = self._make_downlayer(downblock, 512, num_layers[3],\n",
    "                                           stride=2)\n",
    "\n",
    "\n",
    "    def _make_downlayer(self, block, init_channels, num_layer, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != init_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, init_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(init_channels * block.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, init_channels, stride, downsample))\n",
    "        self.in_channels = init_channels * block.expansion\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        output['re-l0-1'] = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        output_layer1 = self.layer1(x)\n",
    "        x = output_layer1[3]\n",
    "        output['re-l1-1'] = output_layer1[0]\n",
    "        output['re-l1-2'] = output_layer1[1]\n",
    "        output['re-l1-2'] = output_layer1[2]\n",
    "        output['re-l1-3'] = output_layer1[3]\n",
    "\n",
    "        output_layer2 = self.layer2(x)\n",
    "        x = output_layer2[3]\n",
    "        output['re-l2-1'] = output_layer2[0]\n",
    "        output['re-l2-2'] = output_layer2[1]\n",
    "        output['re-l2-2'] = output_layer2[2]\n",
    "        output['re-l2-3'] = output_layer2[3]\n",
    "\n",
    "        output_layer3 = self.layer3(x)\n",
    "        x = output_layer3[3]\n",
    "        output['re-l3-1'] = output_layer3[0]\n",
    "        output['re-l3-2'] = output_layer3[1]\n",
    "        output['re-l3-2'] = output_layer3[2]\n",
    "        output['re-l3-3'] = output_layer3[3]\n",
    "\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        output_layer4 = self.layer4(x)\n",
    "        x = output_layer4[3]\n",
    "        output['re-l4-1'] = output_layer4[0]\n",
    "        output['re-l4-2'] = output_layer4[1]\n",
    "        output['re-l4-2'] = output_layer4[2]\n",
    "        output['re-l4-3'] = output_layer4[3]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92105bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### ResNet Decoder Definition #######################################################\n",
    "\n",
    "class ResNet_decoder(nn.Module):\n",
    "    def __init__(self, upblock, num_layers, n_classes):\n",
    "        super(ResNet_decoder, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.uplayer1 = self._make_up_block(\n",
    "            upblock, 512,  num_layers[3], stride=2)\n",
    "        self.uplayer2 = self._make_up_block(\n",
    "            upblock, 256, num_layers[2], stride=2)\n",
    "        self.uplayer3 = self._make_up_block(\n",
    "            upblock, 128, num_layers[1], stride=2)\n",
    "        self.uplayer4 = self._make_up_block(\n",
    "            upblock, 64,  num_layers[0], stride=2)\n",
    "\n",
    "        upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.in_channels,  # 256\n",
    "                               64,\n",
    "                               kernel_size=1, stride=2,\n",
    "                               bias=False, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        self.uplayer_top = DeconvBottleneck(\n",
    "            self.in_channels, 64, 1, 2, upsample)\n",
    "\n",
    "        self.conv1_1 = nn.ConvTranspose2d(64, n_classes, kernel_size=1, stride=1,\n",
    "                                          bias=False)\n",
    "\n",
    "    def _make_up_block(self, block, init_channels, num_layer, stride=1):\n",
    "        upsample = None\n",
    "        # expansion = block.expansion\n",
    "        if stride != 1 or self.in_channels != init_channels * 2:\n",
    "            upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(self.in_channels, init_channels * 2,\n",
    "                                   kernel_size=1, stride=stride,\n",
    "                                   bias=False, output_padding=1),\n",
    "                nn.BatchNorm2d(init_channels * 2),\n",
    "            )\n",
    "        layers = []\n",
    "        for i in range(1, num_layer):\n",
    "            layers.append(block(self.in_channels, init_channels, 4))\n",
    "        layers.append(\n",
    "            block(self.in_channels, init_channels, 2, stride, upsample))\n",
    "        self.in_channels = init_channels * 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, image_size):\n",
    "        x = self.uplayer1(x)\n",
    "        x = self.uplayer2(x)\n",
    "        x = self.uplayer3(x)\n",
    "        x = self.uplayer4(x)\n",
    "\n",
    "        x = self.conv1_1(x, output_size=image_size)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a745218",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We do not need the forward method  \n",
    "\n",
    "class ResNet_autoencoder(nn.Module):\n",
    "    def __init__(self, downblock, upblock, num_layers, n_classes):\n",
    "        super(ResNet_autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = ResNet_encoder(downblock, num_layers)\n",
    "        self.decoder = ResNet_decoder(upblock, num_layers, n_classes)\n",
    "\n",
    "        \n",
    "\n",
    "    def encoder(self, x):\n",
    "        return self.encoder.forward(x)\n",
    "\n",
    "    def decoder(self, x, image_size):\n",
    "        return self.decoder.forward(x, image_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img = x\n",
    "        dict_encoder = self.encoder(x)\n",
    "        # final value of the encoder is stored in entry 're-l4-3'\n",
    "        x = self.decoder(dict_encoder['re-l4-3'], img.size())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae24c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, matrixSize=32):\n",
    "        super(CNN,self).__init__()\n",
    "\n",
    "        self.convs = nn.Sequential(nn.Conv2d(512,256,3,1,1),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(256,128,3,1,1),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.Conv2d(128,matrixSize,3,1,1))\n",
    "\n",
    "        # 32x8x8\n",
    "        self.fc = nn.Linear(matrixSize*matrixSize,matrixSize*matrixSize)\n",
    "        #self.fc = nn.Linear(32*64,256*256)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.convs(x)\n",
    "        # 32x8x8\n",
    "        b,c,h,w = out.size()\n",
    "        out = out.view(b,c,-1)\n",
    "        # 32x64\n",
    "        out = torch.bmm(out,out.transpose(1,2)).div(h*w)\n",
    "        # 32x32\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return self.fc(out)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(VAE,self).__init__()\n",
    "\n",
    "        # 32x8x8\n",
    "        self.encode = nn.Sequential(nn.Linear(512, 2*z_dim),\n",
    "                                    )\n",
    "        self.bn = nn.BatchNorm1d(z_dim)\n",
    "        self.decode = nn.Sequential(nn.Linear(z_dim, 512),\n",
    "                                    nn.BatchNorm1d(512),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(512, 512),\n",
    "                                    )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        mu = self.bn(mu)\n",
    "        std = torch.exp(logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "\n",
    "        return mu + std\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 32x8x8\n",
    "        b,c,h = x.size()\n",
    "        x = x.view(b,-1)\n",
    "\n",
    "        z_q_mu, z_q_logvar = self.encode(x).chunk(2, dim=1)\n",
    "        # reparameterize\n",
    "        z_q = self.reparameterize(z_q_mu, z_q_logvar)\n",
    "        out = self.decode(z_q)\n",
    "        out = out.view(b,c,h)\n",
    "\n",
    "        KL = torch.sum(0.5 * (z_q_mu.pow(2) + z_q_logvar.exp().pow(2) - 1) - z_q_logvar)\n",
    "\n",
    "        return out, KL\n",
    "\n",
    "class MulLayer(nn.Module):\n",
    "    def __init__(self, z_dim, matrixSize=32):\n",
    "        super(MulLayer,self).__init__()\n",
    "        # self.snet = CNN_VAE(layer, z_dim, matrixSize)\n",
    "        self.snet = CNN(matrixSize)\n",
    "        self.cnet = CNN(matrixSize)\n",
    "        self.VAE = VAE(z_dim=z_dim)\n",
    "        self.matrixSize = matrixSize\n",
    "\n",
    "        self.compress = nn.Conv2d(512,matrixSize,1,1,0)\n",
    "        self.unzip = nn.Conv2d(matrixSize,512,1,1,0)\n",
    "\n",
    "        self.transmatrix = None\n",
    "\n",
    "    def forward(self,cF,sF,trans=True):\n",
    "        cb,cc,ch,cw = cF.size()\n",
    "        cFF = cF.view(cb,cc,-1)\n",
    "        cMean = torch.mean(cFF,dim=2,keepdim=True)\n",
    "        cMean = cMean.unsqueeze(3)\n",
    "        cMean = cMean.expand_as(cF)\n",
    "        cF = cF - cMean\n",
    "\n",
    "        sb,sc,sh,sw = sF.size()\n",
    "        sFF = sF.view(sb,sc,-1)\n",
    "        sMean = torch.mean(sFF,dim=2,keepdim=True)\n",
    "        sMean, KL = self.VAE(sMean)\n",
    "        sMean = sMean.unsqueeze(3)\n",
    "        sMeanC = sMean.expand_as(cF)\n",
    "        sMeanS = sMean.expand_as(sF)\n",
    "        sF = sF - sMeanS\n",
    "\n",
    "\n",
    "        compress_content = self.compress(cF)\n",
    "        b,c,h,w = compress_content.size()\n",
    "        compress_content = compress_content.view(b,c,-1)\n",
    "\n",
    "        if(trans):\n",
    "            cMatrix = self.cnet(cF)\n",
    "            sMatrix = self.snet(sF)\n",
    "\n",
    "            sMatrix = sMatrix.view(sMatrix.size(0),self.matrixSize,self.matrixSize)\n",
    "            cMatrix = cMatrix.view(cMatrix.size(0),self.matrixSize,self.matrixSize)\n",
    "            transmatrix = torch.bmm(sMatrix,cMatrix)\n",
    "            transfeature = torch.bmm(transmatrix,compress_content).view(b,c,h,w)\n",
    "            out = self.unzip(transfeature.view(b,c,h,w))\n",
    "            out = out + sMeanC\n",
    "            return out, transmatrix, KL\n",
    "        else:\n",
    "            out = self.unzip(compress_content.view(b,c,h,w))\n",
    "            out = out + cMean\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df7f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################### Training Loop #######################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model - The array is the number of blocks -  and the 3 is (we think) RGB \n",
    "model = ResNet_autoencoder(Bottleneck, DeconvBottleneck, [3, 4, 6, 3], 3).cuda()\n",
    "\n",
    "\n",
    "latent_dim = 33 # dummy value\n",
    "# chose the latent space\n",
    "matrix = MulLayer(z_dim=latent_dim)\n",
    "\n",
    "# Load Dataset\n",
    "\n",
    "# Load a pretrained ResNet 50 \n",
    "pretrained_dict = torch.load('./resnet50-19c8e357.pth')\n",
    "print(\"load pretrained model success\")\n",
    "\n",
    "\n",
    "# Create model dictionary\n",
    "model_dict = model.state_dict()\n",
    "# 1. filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "model_dict.update(pretrained_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# Set all parameters to untrainable\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "################# LOSS & OPTIMIZER #################\n",
    "# Possible entries \n",
    "criterion = LossCriterion(opt.style_layers, opt.content_layers, opt.style_weight, opt.content_weight)\n",
    "optimizer = optim.Adam(matrix.parameters(), opt.lr)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    \n",
    "\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        content, target, style = Variable(batch[0]), Variable(batch[1]), Variable(batch[2])\n",
    "        content = content.cuda()\n",
    "        target = target.cuda()\n",
    "        style = style.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        ### To access encoder and decoder intermediate values \n",
    "        ## x is the input \n",
    "        sF = model.encoder(style)\n",
    "        cF = model.encoder(style)\n",
    "\n",
    "        # possible values in the dictionary of the encoder per layer: \n",
    "        # first relu after the first conolution: 're-l0-1'\n",
    "        # layer 1: 're-l1-1' -> 're-l1-2' -> 're-l1-3' -> 're-l1-4'\n",
    "        # layer 2: 're-l2-1' -> 're-l2-2' -> 're-l2-3' -> 're-l2-4'\n",
    "        # layer 3: 're-l3-1' -> 're-l3-2' -> 're-l3-3' -> 're-l3-4'\n",
    "        # layer 4: 're-l4-1' -> 're-l4-2' -> 're-l4-3' -> 're-l4-4'\n",
    "        # this needs to be carefully chosen \n",
    "        cF_intermediate = cF['re-l1-1']\n",
    "        sF_intermediate = sF['re-l1-1', 're-l1-2']\n",
    "        feature, transmatrix, KL = matrix(cF_intermediate, sF_intermediate)\n",
    "\n",
    "\n",
    "        transfer = model.decoder(feature)\n",
    "\n",
    "\n",
    "        ## Need to find a suitable loss network \n",
    "        #sF_loss = vgg5(style)\n",
    "        #cF_loss = vgg5(content)\n",
    "        #tF = vgg5(transfer)\n",
    "        #loss, styleLoss, contentLoss, KL_loss = criterion(tF, sF_loss, cF_loss, KL)\n",
    "\n",
    "         # backward & optimization\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "\n",
    "        #print(\"===> Epoch[{}]({}/{}): loss: {:.4f} || content: {:.4f} || style: {:.4f} KL: {:.4f}.\".format(epoch, iteration, len(training_data_loader), loss, contentLoss, styleLoss, KL_loss,))\n",
    "\n",
    "    #print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n",
    "\n",
    "    #return content, style, transfer\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
